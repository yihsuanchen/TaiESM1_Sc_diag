{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323157ca",
   "metadata": {},
   "source": [
    "# Program - Read VOCALS-REx GFS SCM forcing data (binary) and save it into a new netCDF file\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "Read VOCALS-REx GFS SCM forcing data and save it into a new netCDF file\n",
    "\n",
    "**Data**\n",
    "\n",
    "- NCAR/UCAR EOL -  VOCALS: NCEP GFS Single Column Model Forcing Data\n",
    "  - https://data.eol.ucar.edu/dataset/89.105\n",
    "- Data access\n",
    "  - ORDER data for delivery by FTP\n",
    "- Documentation\n",
    "  - https://data.eol.ucar.edu/file/download/41B38ABB023/NCEP_GFS_Single_Column_Model_Forcing_Data_for_VOCALS_Rex.pdf\n",
    "\n",
    "**Author:** Yi-Hsuan Chen (yihsuan@umich.edu)\n",
    "\n",
    "**Date:** May 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22208803-04be-4d2f-b899-447072a095af",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e61034-6e49-4ab4-b1cb-a4e61dfbdce9",
   "metadata": {},
   "source": [
    "## read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c093ae2-d706-4d9e-9b74-a5253772e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "####################\n",
    "####################\n",
    "####################\n",
    "def create_return_arrays(num_time_steps, npoint=25, levs=64):\n",
    "\n",
    "    variable_shapes = {\n",
    "\n",
    "    #--- 1d variable, (time)\n",
    "    'date': (num_time_steps),\n",
    "\n",
    "    #--- 1d variable, (station)\n",
    "    'station': (npoint),\n",
    "    'latitude': (npoint),\n",
    "    'longitude': (npoint),\n",
    "\n",
    "    #--- 1d variable, (levs/levs+1)\n",
    "    'sigi': (levs+1),\n",
    "    'ak5': (levs+1),\n",
    "    'bk5': (levs+1),\n",
    "    'sigl': (levs),\n",
    "        \n",
    "    #--- 2d variable, (time, station)\n",
    "    'zsfc': (num_time_steps, npoint),\n",
    "    'psfc': (num_time_steps, npoint),\n",
    "    'dpsdt': (num_time_steps, npoint),\n",
    "    'tsfc': (num_time_steps, npoint),\n",
    "    'u10': (num_time_steps, npoint),\n",
    "    'v10': (num_time_steps, npoint),\n",
    "    't2': (num_time_steps, npoint),\n",
    "    'q2': (num_time_steps, npoint),\n",
    "    'hpbl': (num_time_steps, npoint),\n",
    "        \n",
    "    #--- 3d variable, (time, station, levels)\n",
    "    'u': (num_time_steps, npoint, levs),\n",
    "    'v': (num_time_steps, npoint, levs),\n",
    "    't': (num_time_steps, npoint, levs),\n",
    "    'q': (num_time_steps, npoint, levs),\n",
    "    'p': (num_time_steps, npoint, levs),\n",
    "    'omega': (num_time_steps, npoint, levs),\n",
    "    'dtdt': (num_time_steps, npoint, levs),\n",
    "    'dqdt': (num_time_steps, npoint, levs),\n",
    "}\n",
    "\n",
    "    data_arrays = {var: np.zeros(shape) for var, shape in variable_shapes.items()}\n",
    "\n",
    "    #--- set a new string variable\n",
    "    #new_string_var = [['' for _ in range(npoint)] for _ in range(num_time_steps)]\n",
    "    #data_arrays['date_string'] = new_string_var\n",
    "\n",
    "    # Initialize the 'station' variable as a list of lists (strings)\n",
    "    #data_arrays['station_string'] = [['' for _ in range(1)] for _ in range(npoint)]\n",
    "    \n",
    "    return data_arrays\n",
    "\n",
    "####################\n",
    "####################\n",
    "####################\n",
    "def read_data(filename, \n",
    "              do_print=False):\n",
    "    with open(filename, 'rb') as file:\n",
    "        # Read the header\n",
    "        header_format = '>13i'\n",
    "        header_size = struct.calcsize(header_format)\n",
    "        header_data = struct.unpack(header_format, file.read(header_size))\n",
    "        \n",
    "        # Unpack the header data\n",
    "        uu, hour, month, day, year, nsfc, nflx, nvar, levs, npoint, start_hour, end_hour, step_hour = header_data\n",
    "\n",
    "        if (do_print):\n",
    "            print(f'Hour: {hour}, Month: {month}, Day: {day}, Year: {year}')\n",
    "            print(f'Number of surface variables: {nsfc}')\n",
    "            print(f'Number of flux variables: {nflx}')\n",
    "            print(f'Number of variables for each sounding: {nvar}')\n",
    "            print(f'Number of vertical levels: {levs}')\n",
    "            print(f'Number of station points: {npoint}')\n",
    "            print(f'Starting forecast hour: {start_hour}')\n",
    "            print(f'Ending forecast hour: {end_hour}')\n",
    "            print(f'Forecast output step: {step_hour}')\n",
    "\n",
    "        #--- create return data array\n",
    "        data = create_return_arrays(num_time_steps=1, npoint=npoint, levs=levs)\n",
    "        time_step = 0\n",
    "        \n",
    "        # Skip the first two values before reading sigi\n",
    "        file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "        \n",
    "        # Read the second record of vertical sounding levels\n",
    "        sigi = np.fromfile(file, dtype='>f4', count=levs + 1)\n",
    "        sigl = np.fromfile(file, dtype='>f4', count=levs)\n",
    "        ak5 = np.fromfile(file, dtype='>f4', count=levs + 1)\n",
    "        bk5 = np.fromfile(file, dtype='>f4', count=levs + 1)\n",
    "            \n",
    "        # Calculate the number of time steps\n",
    "        num_time_steps = (end_hour - start_hour) // step_hour + 1\n",
    "\n",
    "        # Loop over station points\n",
    "        for i in range(npoint):\n",
    "\n",
    "            # Skip the first two values before reading surface_data\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "\n",
    "            # Read surface variables\n",
    "            surface_vars_format = f'>{nsfc}f'\n",
    "            #surface_vars_format = f'>50f'\n",
    "            surface_vars_size = struct.calcsize(surface_vars_format)\n",
    "            surface_data = struct.unpack(surface_vars_format, file.read(surface_vars_size))\n",
    "\n",
    "            # Print each element in surface_data\n",
    "            #print(f'Surface Data for Station {i+1}:')                 \n",
    "            if (do_print):\n",
    "                for j, value in enumerate(surface_data):\n",
    "                    print(f'surface_data {j}: {value}')\n",
    "\n",
    "            # Skip the first two values before reading flux_data\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "            \n",
    "            # Read flux type variables if nflx > 0\n",
    "            if nflx > 0:\n",
    "                flux_vars_format = f'>{nflx}f'\n",
    "                flux_vars_size = struct.calcsize(flux_vars_format)\n",
    "                flux_data = struct.unpack(flux_vars_format, file.read(flux_vars_size))\n",
    "\n",
    "                # Print flux data for debugging purposes\n",
    "                if (do_print):\n",
    "                    for j, value in enumerate(flux_data):\n",
    "                        print(f'flux_data {j}: {value}')\n",
    "                        \n",
    "            # Read vertical levels data\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            u = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            v = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            t = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            q = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            p = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            if nvar > 5:\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                omega = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                dtdt = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                dqdt = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            if nvar > 8:\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                cloud_water = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                cloud_water_tend = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                cloud_fraction = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            #--- save variables into data array\n",
    "\n",
    "            str1 = f\"{year}{month}{day:02}{hour:02}\"\n",
    "            data['date'][time_step] = str1\n",
    "            data['station'][i] = f\"{i+1:02}\"\n",
    "\n",
    "            #--- 1d variable \n",
    "            data['latitude'][i] = surface_data[0]\n",
    "            data['longitude'][i] = surface_data[1]\n",
    "            data['sigi'][:] = sigi\n",
    "            data['sigl'][:] = sigl\n",
    "            data['ak5'][:] = ak5\n",
    "            data['bk5'][:] = bk5\n",
    "            \n",
    "            #--- 2d variable, (time, station)\n",
    "            data['zsfc'][time_step, i] = surface_data[2]\n",
    "            data['psfc'][time_step, i] = surface_data[3] / 1000.  # change units from mPa to Pa\n",
    "            data['tsfc'][time_step, i] = surface_data[4]\n",
    "            data['dpsdt'][time_step, i] = surface_data[5]\n",
    "\n",
    "            data['u10'][time_step, i] = flux_data[17]\n",
    "            data['v10'][time_step, i] = flux_data[18]\n",
    "            data['t2'][time_step, i] = flux_data[19]\n",
    "            data['q2'][time_step, i] = flux_data[20]\n",
    "            data['hpbl'][time_step, i] = flux_data[26]\n",
    "\n",
    "            #--- 3d variable, (time, station, levs/levs+1)\n",
    "            data['u'][time_step, i, :] = u\n",
    "            data['v'][time_step, i, :] = v\n",
    "            data['t'][time_step, i, :] = t\n",
    "            data['q'][time_step, i, :] = q\n",
    "            data['p'][time_step, i, :] = p / 1000.  # change units from mPa to Pa\n",
    "            data['omega'][time_step, i, :] = omega / 1000.  # change units from mPa to Pa\n",
    "            data['dtdt'][time_step, i, :] = dtdt\n",
    "            data['dqdt'][time_step, i, :] = dqdt\n",
    "            #data[''][time_step, i, :] = \n",
    "\n",
    "        if (do_print):\n",
    "            print('Sigi:', sigi)\n",
    "            print('Sigl:', sigl)\n",
    "            print('Ak5:', ak5)\n",
    "            print('Bk5:', bk5)\n",
    "            print('u:', u)\n",
    "            print('v:', v)\n",
    "            print('t:', t)\n",
    "            print('q:', q)\n",
    "            print('omega:', omega*1e+5/86400)\n",
    "            print('dtdt', dtdt)\n",
    "            print('dqdt', dtdt)\n",
    "            print('cloud_water', cloud_water)\n",
    "            print('cloud_water_tend', cloud_water_tend)\n",
    "            print('cloud_fraction', cloud_fraction)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "filename = '../original/vocalsgfs.2008101100'\n",
    "\n",
    "#data = read_data(filename, do_print=True)\n",
    "data = read_data(filename, do_print=False)\n",
    "\n",
    "#data['q2']\n",
    "#data['p'][0,0,:]\n",
    "#data['latitude']\n",
    "#print(data['date'].astype(int))\n",
    "#print(data['station'])\n",
    "\n",
    "#data\n",
    "\n",
    "#data_all = create_return_arrays(num_time_steps=2, npoint=25, levs=64)\n",
    "\n",
    "#filename = '../original/vocalsgfs.2008101100'\n",
    "#read_data(filename, data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496fbf26-f4e6-4bf9-972c-736a1f6f5439",
   "metadata": {},
   "source": [
    "## create_xarray_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8d80b0-335a-44ec-9b2c-cec733e11325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def create_xarray_dataset(num_time_steps=122, num_station=25, num_levels=64):\n",
    "    # Define the coordinate arrays\n",
    "    times = np.arange(num_time_steps)\n",
    "    stations = np.arange(1, num_station+1)\n",
    "    levels_mid = np.arange(num_levels)\n",
    "    levels_int = np.arange(num_levels + 1)\n",
    "\n",
    "    # Create the xarray Dataset with coordinates\n",
    "    ds = xr.Dataset(\n",
    "        coords={\n",
    "            'time': ('time', times),\n",
    "            'station': ('station', stations),\n",
    "            'lev_mid': ('lev_mid', levels_mid),\n",
    "            'lev_int': ('lev_int', levels_int),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Define attributes for coordinates\n",
    "    coordinate_attributes = {\n",
    "        'time': {'units': 'hours since 2024-01-01 00:00:00', 'long_name': 'time'},\n",
    "        'station': {'units': '1', 'long_name': 'station index, VOCALS[##]'},\n",
    "        'lev_mid': {'units': '1', 'long_name': 'sigma level at midpoint (start from the near surface level)'},\n",
    "        'lev_int': {'units': '1', 'long_name': 'sigma level at interface (start from the near surface level)'},\n",
    "    }\n",
    "\n",
    "    # Assign attributes to coordinates\n",
    "    for coord_name, attrs in coordinate_attributes.items():\n",
    "        ds.coords[coord_name].attrs = attrs\n",
    "\n",
    "    # Define the dimension tuples\n",
    "    time_only = ('time',)\n",
    "    station_only = ('station',)\n",
    "    lev_mid_only = ('lev_mid',)\n",
    "    lev_int_only = ('lev_int',)\n",
    "    time_station = ('time', 'station')\n",
    "    time_station_levmid = ('time', 'station', 'lev_mid')\n",
    "    time_station_levint = ('time', 'station', 'lev_int')\n",
    "\n",
    "    # Define the data variables and their dimensions\n",
    "    variable_specs = {\n",
    "        'date': time_only,\n",
    "        'latitude': station_only,\n",
    "        'longitude': station_only,\n",
    "        'sigl': lev_mid_only,\n",
    "        'sigi': lev_int_only,\n",
    "        'ak5': lev_int_only,\n",
    "        'bk5': lev_int_only,\n",
    "        'zsfc': time_station,\n",
    "        'psfc': time_station,\n",
    "        'tsfc': time_station,\n",
    "        'u10': time_station,\n",
    "        'v10': time_station,\n",
    "        't2': time_station,\n",
    "        'q2': time_station,\n",
    "        'hpbl': time_station,\n",
    "        'u': time_station_levmid,\n",
    "        'v': time_station_levmid,\n",
    "        't': time_station_levmid,\n",
    "        'q': time_station_levmid,\n",
    "        'p': time_station_levmid,\n",
    "        'omega': time_station_levmid,\n",
    "        'dtdt': time_station_levmid,\n",
    "        'dqdt': time_station_levmid,\n",
    "    }\n",
    "\n",
    "    # Define attributes for each variable\n",
    "    variable_attributes = {\n",
    "        'date': {'units': 'none', 'long_name': 'YYYYMMDDHH (UTC)'},\n",
    "        'latitude': {'units': 'degrees_north', 'long_name': 'latitude of station'},\n",
    "        'longitude': {'units': 'degrees_east', 'long_name': 'longitude of station'},\n",
    "        'zsfc': {'units': 'm', 'long_name': 'model surface height for the station'},\n",
    "        'psfc': {'units': 'Pa', 'long_name': 'model surface pressure for the station'},\n",
    "        'tsfc': {'units': 'K', 'long_name': 'model surface temperature'},\n",
    "        'u10': {'units': 'm/s', 'long_name': 'model derived 10-meter zonal wind'},\n",
    "        'v10': {'units': 'm/s', 'long_name': 'model derived 10-meter meridional wind'},\n",
    "        't2': {'units': 'K', 'long_name': 'model derived 2-meter temperature'},\n",
    "        'q2': {'units': 'kg/kg', 'long_name': 'model derived 2-meter specific humidity'},\n",
    "        'hpbl': {'units': 'm', 'long_name': 'model diagnosed planetary boundary layer depth'},\n",
    "        'u': {'units': 'm/s', 'long_name': 'model zonal wind velocity'},\n",
    "        'v': {'units': 'm/s', 'long_name': 'model meridional wind velocity'},\n",
    "        't': {'units': 'K', 'long_name': 'model temperature'},\n",
    "        'q': {'units': 'kg/kg', 'long_name': 'model specific humidity'},\n",
    "        'p': {'units': 'Pa', 'long_name': 'model pressure'},\n",
    "        'omega': {'units': 'Pa/s', 'long_name': 'model derived omega'},\n",
    "        'dtdt': {'units': 'K/s', 'long_name': 'model derived dtdt (advection)'},\n",
    "        'dqdt': {'units': 'kg/kg/s', 'long_name': 'model derived dqdt (advection)'},\n",
    "        'sigl': {'units': '1', 'long_name': 'model sigma level at midpoint (count upward)'},\n",
    "        'sigi': {'units': '1', 'long_name': 'model sigma level at interface (count upward)'},\n",
    "        'ak5': {'units': '1', 'long_name': 'A-coefficient'},\n",
    "        'bk5': {'units': '1', 'long_name': 'B-coefficient'},\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Add the data variables to the Dataset\n",
    "    for var_name, dims in variable_specs.items():\n",
    "        shape = tuple(ds.coords[dim].size for dim in dims)\n",
    "        ds[var_name] = xr.DataArray(np.random.rand(*shape), dims=dims)\n",
    "        ds[var_name].attrs = variable_attributes.get(var_name, {})\n",
    "\n",
    "    # Define global attributes\n",
    "    global_attributes = {\n",
    "        'title': 'NCAP GFS Single Column Model Forcing Data for VOCALS-Rex',\n",
    "        'data_source': 'NCAR/UCAR EOL - VOCALS: NCEP GFS Single Column Model Forcing Data, https://data.eol.ucar.edu/dataset/89.105',\n",
    "        'data_reference': 'https://data.eol.ucar.edu/file/download/41B38ABB023/NCEP_GFS_Single_Column_Model_Forcing_Data_for_VOCALS_Rex.pdf',\n",
    "        'history': f'Created on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n",
    "        'author': 'Yi-Hsuan Chen (yihsuanc@gate.sinica.edu.tw)'\n",
    "    }\n",
    "\n",
    "    # Assign global attributes to the Dataset\n",
    "    ds.attrs = global_attributes\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# Example usage\n",
    "ds = create_xarray_dataset()\n",
    "#ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3947421-2c7e-4f00-a6ee-8688dd1b4746",
   "metadata": {},
   "source": [
    "## process_files_in_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25b58e5-1c62-4682-a721-9b5a434aa542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def process_files_in_directory(directory='/lfs/home/yihsuanc/data/data.TaiESM1_scm/iop/VOCALS-REx/original/'):\n",
    "\n",
    "    file_names = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('vocalsgfs'):\n",
    "            #print(filename)\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            file_names.append(file_path)\n",
    "    return sorted(file_names)\n",
    "\n",
    "#file_names = process_files_in_directory()\n",
    "#for ff in file_names:\n",
    "#    print(ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff51050-e94a-4ab4-934c-fe79fb30f675",
   "metadata": {},
   "source": [
    "## read_data_all_to_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68223b30-fb92-4329-aca0-cbd0859b11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_all_to_ds():\n",
    "    #--- get all vocalsgfs file paths\n",
    "    file_names = process_files_in_directory()\n",
    "    \n",
    "    #--- Create an xarray dataset\n",
    "    ds_all = create_xarray_dataset()\n",
    "    \n",
    "    #--- process file_names  and then save into ds_all\n",
    "    for i, ff in enumerate(file_names):\n",
    "        #print(f'Read [{i}, {ff}]')\n",
    "        data = read_data(ff)    \n",
    "    \n",
    "        for var_name in data.keys():\n",
    "            if var_name in ds_all.variables:\n",
    "                ndim = data[var_name].ndim\n",
    "                #print(f\"Variable: {var_name}, ndim: {ndim}\")\n",
    "        \n",
    "                if (ndim == 1 and var_name == 'date'):\n",
    "                   ds_all[var_name][i] = data[var_name][0]\n",
    "                if (ndim == 2):\n",
    "                   ds_all[var_name][i,:] = data[var_name][0,:]\n",
    "                elif (ndim == 3):\n",
    "                   ds_all[var_name][i,:,:] = data[var_name][0,:,:]\n",
    "    \n",
    "        var_1d = ['latitude', 'longitude', 'sigl', 'sigi', 'ak5', 'bk5']\n",
    "        for var_name in var_1d:\n",
    "            ds_all[var_name][:] = data[var_name][:]\n",
    "    \n",
    "    #--- change coordinate\n",
    "    ds_all = ds_all.assign_coords(time=ds_all['date'].values)\n",
    "    ds_all.coords['time'].attrs = ds_all['date'].attrs\n",
    "    \n",
    "    ds_all = ds_all.assign_coords(lev_mid=ds_all['sigl'][:].values)\n",
    "    ds_all.coords['lev_mid'].attrs = ds_all['sigl'].attrs\n",
    "    \n",
    "    ds_all = ds_all.assign_coords(lev_int=ds_all['sigi'][:].values)\n",
    "    ds_all.coords['lev_int'].attrs = ds_all['sigi'].attrs\n",
    "\n",
    "    return ds_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c202f-9262-4966-95dc-3a47e0ba9a24",
   "metadata": {},
   "source": [
    "# Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f66c09-8eb9-4063-b391-670913bf5b14",
   "metadata": {},
   "source": [
    "## Read a single date, all 25 stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3abfa1a-a006-436a-839a-4021e351c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': (1,), 'station': (25,), 'latitude': (25,), 'longitude': (25,), 'sigi': (65,), 'ak5': (65,), 'bk5': (65,), 'sigl': (64,), 'zsfc': (1, 25), 'psfc': (1, 25), 'dpsdt': (1, 25), 'tsfc': (1, 25), 'u10': (1, 25), 'v10': (1, 25), 't2': (1, 25), 'q2': (1, 25), 'hpbl': (1, 25), 'u': (1, 25, 64), 'v': (1, 25, 64), 't': (1, 25, 64), 'q': (1, 25, 64), 'p': (1, 25, 64), 'omega': (1, 25, 64), 'dtdt': (1, 25, 64), 'dqdt': (1, 25, 64)}\n"
     ]
    }
   ],
   "source": [
    "filename = '../original/vocalsgfs.2008100100'\n",
    "data1 = read_data(filename, do_print=False)\n",
    "\n",
    "variable_dimensions = {var: arr.shape for var, arr in data1.items()}\n",
    "print(variable_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5c58c-48f7-44c0-bbf6-1c47c9609e77",
   "metadata": {},
   "source": [
    "## Read all dates, all 25 stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97a6a4d-1f08-4a00-a703-35667e08be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (time: 122, station: 25, lev_mid: 64, lev_int: 65)\n",
      "Coordinates:\n",
      "  * time       (time) float64 2.008e+09 2.008e+09 ... 2.008e+09 2.008e+09\n",
      "  * station    (station) int64 1 2 3 4 5 6 7 8 9 ... 17 18 19 20 21 22 23 24 25\n",
      "  * lev_mid    (lev_mid) float64 0.9973 0.9917 0.9852 ... 0.00101 0.0003212\n",
      "  * lev_int    (lev_int) float64 1.0 0.9947 0.9886 ... 0.001378 0.0006425 0.0\n",
      "Data variables: (12/23)\n",
      "    date       (time) float64 2.008e+09 2.008e+09 ... 2.008e+09 2.008e+09\n",
      "    latitude   (station) float64 -20.0 -20.0 -20.0 -20.0 ... -28.0 -30.0 -23.5\n",
      "    longitude  (station) float64 -95.0 -92.5 -90.0 -87.25 ... -85.0 -85.0 -70.0\n",
      "    sigl       (lev_mid) float64 0.9973 0.9917 0.9852 ... 0.00101 0.0003212\n",
      "    sigi       (lev_int) float64 1.0 0.9947 0.9886 ... 0.001378 0.0006425 0.0\n",
      "    ak5        (lev_int) float64 0.0 0.06425 0.1378 0.222 ... 0.000575 0.0 0.0\n",
      "    ...         ...\n",
      "    t          (time, station, lev_mid) float64 292.8 292.4 ... 264.2 236.2\n",
      "    q          (time, station, lev_mid) float64 0.01001 0.00981 ... 6.779e-07\n",
      "    p          (time, station, lev_mid) float64 1.019e+05 1.013e+05 ... 26.66\n",
      "    omega      (time, station, lev_mid) float64 0.007672 0.006659 ... 0.0001691\n",
      "    dtdt       (time, station, lev_mid) float64 4.564e-05 ... 0.0003126\n",
      "    dqdt       (time, station, lev_mid) float64 -1.243e-08 ... 1.487e-12\n",
      "Attributes:\n",
      "    title:           NCAP GFS Single Column Model Forcing Data for VOCALS-Rex\n",
      "    data_source:     NCAR/UCAR EOL - VOCALS: NCEP GFS Single Column Model For...\n",
      "    data_reference:  https://data.eol.ucar.edu/file/download/41B38ABB023/NCEP...\n",
      "    history:         Created on 2024-06-02 09:30:33\n",
      "    author:          Yi-Hsuan Chen (yihsuanc@gate.sinica.edu.tw)\n"
     ]
    }
   ],
   "source": [
    "ds_all = read_data_all_to_ds()\n",
    "print(ds_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c7c9f-7abb-46dc-a338-6f1ca38c4149",
   "metadata": {},
   "source": [
    "## Check combined dataset with inividual one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bdb8aee-2d44-4c12-839b-4d874f76f164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_all:  -23.5\n",
      "single:  -23.5\n",
      "ds_all:  -70.0\n",
      "single:  -70.0\n",
      "psfc\n",
      "  ds_all:  90640.944\n",
      "  ds_all - single:  0.0\n",
      "tsfc\n",
      "  ds_all:  -0.00035668700002133846\n",
      "  ds_all - single:  0.0\n",
      "t2\n",
      "  ds_all:  292.721435546875\n",
      "  ds_all - single:  0.0\n",
      "q2\n",
      "  ds_all:  0.004474032204598188\n",
      "  ds_all - single:  0.0\n",
      "t\n",
      "  ds_all:  [292.14932251 291.72488403 291.35308838 291.04360962 290.90640259\n",
      " 290.78744507 290.56854248 290.29437256 289.99526978 289.60586548\n",
      " 289.03372192 288.18099976 286.92849731 285.31399536 283.59762573\n",
      " 281.92935181 280.4543457  279.50125122 278.35479736 276.55459595\n",
      " 274.86151123 272.89373779 270.11022949 267.07138062 262.91268921\n",
      " 257.77105713 253.14880371 248.43051147 244.07765198 240.12701416\n",
      " 234.32852173 229.38879395 224.26901245 219.63294983 214.94517517\n",
      " 210.65609741 207.09249878 202.934021   199.91612244 198.14616394\n",
      " 199.56474304 201.57905579 204.05007935 208.64776611 210.79771423\n",
      " 210.78414917 211.54963684 212.1673584  212.84555054 215.31077576\n",
      " 216.98806763 217.46577454 220.84065247 223.87338257 226.48335266\n",
      " 228.83876038 236.34370422 241.30026245 239.95256042 237.4896698\n",
      " 242.53117371 252.443573   262.01605225 243.41485596]\n",
      "  ds_all - single:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "u\n",
      "  ds_all:  [-0.99239963 -1.30918658 -1.66366386 -2.04158473 -2.29049492 -2.40995598\n",
      " -2.52280498 -2.65560389 -2.86899495 -3.15492439 -3.46368718 -3.69775105\n",
      " -3.82395029 -3.89841175 -3.94418073 -3.82905197 -3.23392868 -1.79458058\n",
      "  0.35378146  2.21106505  3.7966876   5.81643438  7.70186758  9.76953983\n",
      " 10.6574688  11.49756718 15.65471649 16.96555519 18.03472328 16.14972878\n",
      " 15.62488365 17.6999855  19.34987068 20.69078445 22.60853577 19.77975655\n",
      " 19.99984169 23.02916336 20.20534325 20.20891953 19.15296745 12.02206802\n",
      " 11.70267582  2.95818281 -0.18606143 -1.50022578  2.69443846 -1.41959739\n",
      " -4.20368576 -4.44826555 -2.90532255 -0.77450037  1.32833946  3.99282742\n",
      "  6.51519632  6.37967682  9.22041416 12.47566032 11.80978203  9.35128021\n",
      "  1.80165601  7.10229015 18.29995918 17.69004059]\n",
      "  ds_all - single:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "v\n",
      "  ds_all:  [-2.51071095 -2.60626793 -2.64847875 -2.61043692 -2.45262313 -2.18763137\n",
      " -1.87034559 -1.54236567 -1.33467686 -1.41339409 -1.874511   -2.63024545\n",
      " -3.57069397 -4.57547235 -5.51269627 -6.18912411 -6.25276852 -4.59445906\n",
      " -1.69629204 -0.89173847 -0.83730513  0.4244217   1.89731646  2.24952745\n",
      "  2.84479856  3.33325958  4.61425066  3.32217765  5.30440903  4.29538202\n",
      "  3.14006138  1.63440526  0.10854103 -1.47021508 -0.57181191  2.65364766\n",
      " -0.85657191  1.17323971 -0.24300979 -4.00425291  1.05138505  5.21202183\n",
      "  2.3252461   5.01154804  1.65552187 -0.74269068  0.31817168  1.68433261\n",
      " -0.07671319  0.79976946  0.50889587 -0.76341128 -0.96904063 -4.01612949\n",
      "  0.26475349 -1.87123728  1.03680336  4.32070875 -1.94547439  6.47546148\n",
      "  4.46069574  0.15495342 -9.14954758 -7.59654665]\n",
      "  ds_all - single:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "q\n",
      "  ds_all:  [ 4.34680795e-03  4.33145883e-03  4.31470526e-03  4.29099798e-03\n",
      "  4.20615124e-03  4.07994585e-03  3.91310407e-03  3.66316619e-03\n",
      "  3.35542858e-03  3.10686789e-03  2.97341915e-03  2.96076946e-03\n",
      "  3.12443497e-03  3.44432634e-03  3.67739866e-03  3.63210402e-03\n",
      "  2.91674607e-03  1.33266835e-03  4.77322174e-04  6.61377388e-04\n",
      "  5.57789521e-04  2.77588639e-04  1.74503526e-04  1.91157291e-04\n",
      "  2.23312352e-04  2.12115221e-04  3.14139936e-04  1.43568235e-04\n",
      "  3.68784422e-05  2.56881503e-05  1.00711804e-04  3.37909260e-05\n",
      "  4.17274932e-05  2.95973496e-05  2.39037654e-05  1.18204434e-05\n",
      "  8.09894209e-06  4.11517749e-06  2.82600968e-06  2.44648209e-06\n",
      " -1.99482724e-06 -6.00334079e-06 -5.67173402e-06 -6.69899373e-06\n",
      " -9.45312058e-06 -9.59509907e-06 -1.10765541e-05 -8.85420104e-06\n",
      " -6.36387222e-06 -5.03769115e-06 -3.92218271e-06 -2.77841764e-06\n",
      " -1.79889673e-06 -1.75455432e-06 -1.48314825e-07  1.80506717e-07\n",
      "  2.05633924e-07 -4.59376707e-07 -3.22092205e-07  1.56553668e-07\n",
      "  2.76738064e-07  4.94254721e-07  4.47050581e-07  7.50586082e-07]\n",
      "  ds_all - single:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "p\n",
      "  ds_all:  [9.03993672e+04 8.98841797e+04 8.93010781e+04 8.86426328e+04\n",
      " 8.79009844e+04 8.70678906e+04 8.61346719e+04 8.50923672e+04\n",
      " 8.39319141e+04 8.26442969e+04 8.12208750e+04 7.96537422e+04\n",
      " 7.79360312e+04 7.60624375e+04 7.40296328e+04 7.18367891e+04\n",
      " 6.94860000e+04 6.69826953e+04 6.43359336e+04 6.15585195e+04\n",
      " 5.86669766e+04 5.56812852e+04 5.26244062e+04 4.95215859e+04\n",
      " 4.63995273e+04 4.32854102e+04 4.02059062e+04 3.71862266e+04\n",
      " 3.42492617e+04 3.14149199e+04 2.86996348e+04 2.61161328e+04\n",
      " 2.36734551e+04 2.13778184e+04 1.92332285e+04 1.72411504e+04\n",
      " 1.54005898e+04 1.37085586e+04 1.21605566e+04 1.07510039e+04\n",
      " 9.47362598e+03 8.32179590e+03 7.28879980e+03 6.36744092e+03\n",
      " 5.54891748e+03 4.82345166e+03 4.18168848e+03 3.61492847e+03\n",
      " 3.11515186e+03 2.67502148e+03 2.28786768e+03 1.94765942e+03\n",
      " 1.64897205e+03 1.38694324e+03 1.15723010e+03 9.55967651e+02\n",
      " 7.79722534e+02 6.25452393e+02 4.90464691e+02 3.72378479e+02\n",
      " 2.69083801e+02 1.78689392e+02 9.93825912e+01 2.66594086e+01]\n",
      "  ds_all - single:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def check_ds_all_with_inidividual(date, station):\n",
    "\n",
    "    #--- combined dataset\n",
    "    ds_all = read_data_all_to_ds()\n",
    "    ds_all.sel(time=date, station=station)\n",
    "    \n",
    "    #--- individual data array\n",
    "    filename = f'../original/vocalsgfs.{date}'\n",
    "    data2 = read_data(filename, do_print=False)\n",
    "    \n",
    "    #--- check lat & lon\n",
    "    var_1d = ['latitude', 'longitude']\n",
    "    for var_name in var_1d:\n",
    "        print('ds_all: ', ds_all[var_name].sel(station=station).values)\n",
    "        print('single: ', data2[var_name][station-1])\n",
    "    \n",
    "    #--- check 2d variable (time, station)\n",
    "    var_2d = ['psfc','tsfc','t2','q2']\n",
    "    for var_name in var_2d:\n",
    "        print(var_name)\n",
    "    \n",
    "        vv1 = ds_all[var_name].sel(time=date, station=station)\n",
    "        vv2 = data2[var_name][0, station-1]\n",
    "        vv1m2 = vv1 - vv2\n",
    "        print('  ds_all: ', vv1.values)\n",
    "        print('  ds_all - single: ', vv1m2.values)\n",
    "        #print('ds_all: ', ds_all[var_name].sel(time=date, station=16).values)\n",
    "        #print('single: ', data2[var_name][0, station-1, :])\n",
    "    \n",
    "    #--- check 3d variable (time, station, levs)\n",
    "    var_nd = ['t','u','v','q','p']\n",
    "    for var_name in var_nd:\n",
    "        print(var_name)\n",
    "    \n",
    "        vv1 = ds_all[var_name].sel(time=date, station=station)\n",
    "        vv2 = data2[var_name][0, station-1, :]\n",
    "        vv1m2 = vv1 - vv2\n",
    "        print('  ds_all: ', vv1.values)\n",
    "        print('  ds_all - single: ', vv1m2.values)\n",
    "\n",
    "#--- example\n",
    "date = 2008101112\n",
    "station = 25\n",
    "check_ds_all_with_inidividual(date=date, station=station)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f224e3-1785-41b1-b728-6486a45675aa",
   "metadata": {},
   "source": [
    "# Save the dataset into a new netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522db1fe-af5f-4e6b-aef9-5e38e2b5424b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6095ff-1bfe-45af-b104-2ce7258e35e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139e25b-994c-4be4-89bb-65946fc223ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
