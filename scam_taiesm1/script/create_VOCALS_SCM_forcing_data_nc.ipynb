{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323157ca",
   "metadata": {},
   "source": [
    "# Program - Read VOCALS-REx GFS SCM forcing data (binary) and save it into a new netCDF file\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "Read VOCALS-REx GFS SCM forcing data and save it into a new netCDF file\n",
    "\n",
    "**Data**\n",
    "\n",
    "- NCAR/UCAR EOL -  VOCALS: NCEP GFS Single Column Model Forcing Data\n",
    "  - https://data.eol.ucar.edu/dataset/89.105\n",
    "- Data access\n",
    "  - ORDER data for delivery by FTP\n",
    "- Documentation\n",
    "  - https://data.eol.ucar.edu/file/download/41B38ABB023/NCEP_GFS_Single_Column_Model_Forcing_Data_for_VOCALS_Rex.pdf\n",
    "\n",
    "**Author:** Yi-Hsuan Chen (yihsuan@umich.edu)\n",
    "\n",
    "**Date:** May 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22208803-04be-4d2f-b899-447072a095af",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e61034-6e49-4ab4-b1cb-a4e61dfbdce9",
   "metadata": {},
   "source": [
    "## read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9c093ae2-d706-4d9e-9b74-a5253772e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "####################\n",
    "####################\n",
    "####################\n",
    "def create_return_arrays(num_time_steps, npoint=25, levs=64):\n",
    "\n",
    "    variable_shapes = {\n",
    "\n",
    "    #--- 1d variable, (time)\n",
    "    'date': (num_time_steps),\n",
    "\n",
    "    #--- 1d variable, (station)\n",
    "    'station': (npoint),\n",
    "    'latitude': (npoint),\n",
    "    'longitude': (npoint),\n",
    "\n",
    "    #--- 1d variable, (levs/levs+1)\n",
    "    'sigi': (levs+1),\n",
    "    'ak5': (levs+1),\n",
    "    'bk5': (levs+1),\n",
    "    'sigl': (levs),\n",
    "        \n",
    "    #--- 2d variable, (time, station)\n",
    "    'zsfc': (num_time_steps, npoint),\n",
    "    'psfc': (num_time_steps, npoint),\n",
    "    'tsfc': (num_time_steps, npoint),\n",
    "    'u10': (num_time_steps, npoint),\n",
    "    'v10': (num_time_steps, npoint),\n",
    "    't2': (num_time_steps, npoint),\n",
    "    'q2': (num_time_steps, npoint),\n",
    "    'hpbl': (num_time_steps, npoint),\n",
    "        \n",
    "    #--- 3d variable, (time, station, levels)\n",
    "    'u': (num_time_steps, npoint, levs),\n",
    "    'v': (num_time_steps, npoint, levs),\n",
    "    't': (num_time_steps, npoint, levs),\n",
    "    'q': (num_time_steps, npoint, levs),\n",
    "    'p': (num_time_steps, npoint, levs),\n",
    "    'omega': (num_time_steps, npoint, levs),\n",
    "    'dtdt': (num_time_steps, npoint, levs),\n",
    "    'dqdt': (num_time_steps, npoint, levs),\n",
    "}\n",
    "\n",
    "    data_arrays = {var: np.zeros(shape) for var, shape in variable_shapes.items()}\n",
    "\n",
    "    #--- set a new string variable\n",
    "    #new_string_var = [['' for _ in range(npoint)] for _ in range(num_time_steps)]\n",
    "    #data_arrays['date_string'] = new_string_var\n",
    "\n",
    "    # Initialize the 'station' variable as a list of lists (strings)\n",
    "    #data_arrays['station_string'] = [['' for _ in range(1)] for _ in range(npoint)]\n",
    "    \n",
    "    return data_arrays\n",
    "\n",
    "####################\n",
    "####################\n",
    "####################\n",
    "def read_data(filename, \n",
    "              do_print=False):\n",
    "    with open(filename, 'rb') as file:\n",
    "        # Read the header\n",
    "        header_format = '>13i'\n",
    "        header_size = struct.calcsize(header_format)\n",
    "        header_data = struct.unpack(header_format, file.read(header_size))\n",
    "        \n",
    "        # Unpack the header data\n",
    "        uu, hour, month, day, year, nsfc, nflx, nvar, levs, npoint, start_hour, end_hour, step_hour = header_data\n",
    "\n",
    "        if (do_print):\n",
    "            print(f'Hour: {hour}, Month: {month}, Day: {day}, Year: {year}')\n",
    "            print(f'Number of surface variables: {nsfc}')\n",
    "            print(f'Number of flux variables: {nflx}')\n",
    "            print(f'Number of variables for each sounding: {nvar}')\n",
    "            print(f'Number of vertical levels: {levs}')\n",
    "            print(f'Number of station points: {npoint}')\n",
    "            print(f'Starting forecast hour: {start_hour}')\n",
    "            print(f'Ending forecast hour: {end_hour}')\n",
    "            print(f'Forecast output step: {step_hour}')\n",
    "\n",
    "        #--- create return data array\n",
    "        data = create_return_arrays(num_time_steps=1, npoint=npoint, levs=levs)\n",
    "        time_step = 0\n",
    "        \n",
    "        # Skip the first two values before reading sigi\n",
    "        file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "        \n",
    "        # Read the second record of vertical sounding levels\n",
    "        sigi = np.fromfile(file, dtype='>f4', count=levs + 1)\n",
    "        sigl = np.fromfile(file, dtype='>f4', count=levs)\n",
    "        ak5 = np.fromfile(file, dtype='>f4', count=levs + 1)\n",
    "        bk5 = np.fromfile(file, dtype='>f4', count=levs + 1)\n",
    "            \n",
    "        # Calculate the number of time steps\n",
    "        num_time_steps = (end_hour - start_hour) // step_hour + 1\n",
    "\n",
    "        # Loop over station points\n",
    "        for i in range(npoint):\n",
    "\n",
    "            # Skip the first two values before reading surface_data\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "\n",
    "            # Read surface variables\n",
    "            surface_vars_format = f'>{nsfc}f'\n",
    "            #surface_vars_format = f'>50f'\n",
    "            surface_vars_size = struct.calcsize(surface_vars_format)\n",
    "            surface_data = struct.unpack(surface_vars_format, file.read(surface_vars_size))\n",
    "\n",
    "            # Print each element in surface_data\n",
    "            #print(f'Surface Data for Station {i+1}:')                 \n",
    "            if (do_print):\n",
    "                for j, value in enumerate(surface_data):\n",
    "                    print(f'surface_data {j}: {value}')\n",
    "\n",
    "            # Skip the first two values before reading flux_data\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "            \n",
    "            # Read flux type variables if nflx > 0\n",
    "            if nflx > 0:\n",
    "                flux_vars_format = f'>{nflx}f'\n",
    "                flux_vars_size = struct.calcsize(flux_vars_format)\n",
    "                flux_data = struct.unpack(flux_vars_format, file.read(flux_vars_size))\n",
    "\n",
    "                # Print flux data for debugging purposes\n",
    "                if (do_print):\n",
    "                    for j, value in enumerate(flux_data):\n",
    "                        print(f'flux_data {j}: {value}')\n",
    "                        \n",
    "            # Read vertical levels data\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            u = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            v = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            t = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            q = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            p = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            if nvar > 5:\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                omega = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                dtdt = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                dqdt = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            if nvar > 8:\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                cloud_water = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                cloud_water_tend = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                cloud_fraction = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            #--- save variables into data array\n",
    "\n",
    "            str1 = f\"{year}{month}{day:02}{hour:02}\"\n",
    "            data['date'][time_step] = str1\n",
    "            data['station'][i] = f\"{i+1:02}\"\n",
    "\n",
    "            #--- 1d variable \n",
    "            data['latitude'][i] = surface_data[0]\n",
    "            data['longitude'][i] = surface_data[1]\n",
    "            data['sigi'][:] = sigi\n",
    "            data['sigl'][:] = sigl\n",
    "            data['ak5'][:] = ak5\n",
    "            data['bk5'][:] = bk5\n",
    "            \n",
    "            #--- 2d variable, (time, station)\n",
    "            data['zsfc'][time_step, i] = surface_data[2]\n",
    "            data['psfc'][time_step, i] = surface_data[3]\n",
    "            data['tsfc'][time_step, i] = surface_data[4]\n",
    "            \n",
    "            data['u10'][time_step, i] = flux_data[17]\n",
    "            data['v10'][time_step, i] = flux_data[18]\n",
    "            data['t2'][time_step, i] = flux_data[19]\n",
    "            data['q2'][time_step, i] = flux_data[20]\n",
    "            data['hpbl'][time_step, i] = flux_data[26]\n",
    "\n",
    "            #--- 3d variable, (time, station, levs/levs+1)\n",
    "            data['u'][time_step, i, :] = u\n",
    "            data['v'][time_step, i, :] = v\n",
    "            data['t'][time_step, i, :] = t\n",
    "            data['q'][time_step, i, :] = q\n",
    "            data['p'][time_step, i, :] = p\n",
    "            data['omega'][time_step, i, :] = omega\n",
    "            data['dtdt'][time_step, i, :] = dtdt\n",
    "            data['dqdt'][time_step, i, :] = dqdt\n",
    "            #data[''][time_step, i, :] = \n",
    "\n",
    "        if (do_print):\n",
    "            print('Sigi:', sigi)\n",
    "            print('Sigl:', sigl)\n",
    "            print('Ak5:', ak5)\n",
    "            print('Bk5:', bk5)\n",
    "            print('u:', u)\n",
    "            print('v:', v)\n",
    "            print('t:', t)\n",
    "            print('q:', q)\n",
    "            print('omega:', omega*1e+5/86400)\n",
    "            print('dtdt', dtdt)\n",
    "            print('dqdt', dtdt)\n",
    "            print('cloud_water', cloud_water)\n",
    "            print('cloud_water_tend', cloud_water_tend)\n",
    "            print('cloud_fraction', cloud_fraction)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "filename = '../original/vocalsgfs.2008101100'\n",
    "\n",
    "#data = read_data(filename, do_print=True)\n",
    "data = read_data(filename, do_print=False)\n",
    "\n",
    "#data['q2']\n",
    "#data['p'][0,11,:]\n",
    "#data['latitude']\n",
    "#print(data['date'].astype(int))\n",
    "#print(data['station'])\n",
    "\n",
    "#data\n",
    "\n",
    "#data_all = create_return_arrays(num_time_steps=2, npoint=25, levs=64)\n",
    "\n",
    "#filename = '../original/vocalsgfs.2008101100'\n",
    "#read_data(filename, data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496fbf26-f4e6-4bf9-972c-736a1f6f5439",
   "metadata": {},
   "source": [
    "## create_xarray_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fa8d80b0-335a-44ec-9b2c-cec733e11325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def create_xarray_dataset(num_time_steps=122, num_station=25, num_levels=64):\n",
    "    # Define the coordinate arrays\n",
    "    times = np.arange(num_time_steps)\n",
    "    stations = np.arange(1, num_station+1)\n",
    "    levels_mid = np.arange(num_levels)\n",
    "    levels_int = np.arange(num_levels + 1)\n",
    "\n",
    "    # Create the xarray Dataset with coordinates\n",
    "    ds = xr.Dataset(\n",
    "        coords={\n",
    "            'time': ('time', times),\n",
    "            'station': ('station', stations),\n",
    "            'lev_mid': ('lev_mid', levels_mid),\n",
    "            'lev_int': ('lev_int', levels_int),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Define attributes for coordinates\n",
    "    coordinate_attributes = {\n",
    "        'time': {'units': 'hours since 2024-01-01 00:00:00', 'long_name': 'time'},\n",
    "        'station': {'units': '1', 'long_name': 'station index, VOCALS[##]'},\n",
    "        'lev_mid': {'units': '1', 'long_name': 'sigma level at midpoint (start from the near surface level)'},\n",
    "        'lev_int': {'units': '1', 'long_name': 'sigma level at interface (start from the near surface level)'},\n",
    "    }\n",
    "\n",
    "    # Assign attributes to coordinates\n",
    "    for coord_name, attrs in coordinate_attributes.items():\n",
    "        ds.coords[coord_name].attrs = attrs\n",
    "\n",
    "    # Define the dimension tuples\n",
    "    time_only = ('time',)\n",
    "    station_only = ('station',)\n",
    "    lev_mid_only = ('lev_mid',)\n",
    "    lev_int_only = ('lev_int',)\n",
    "    time_station = ('time', 'station')\n",
    "    time_station_levmid = ('time', 'station', 'lev_mid')\n",
    "    time_station_levint = ('time', 'station', 'lev_int')\n",
    "\n",
    "    # Define the data variables and their dimensions\n",
    "    variable_specs = {\n",
    "        'date': time_only,\n",
    "        'latitude': station_only,\n",
    "        'longitude': station_only,\n",
    "        'sigl': lev_mid_only,\n",
    "        'sigi': lev_int_only,\n",
    "        'ak5': lev_int_only,\n",
    "        'bk5': lev_int_only,\n",
    "        'zsfc': time_station,\n",
    "        'psfc': time_station,\n",
    "        'tsfc': time_station,\n",
    "        'u10': time_station,\n",
    "        'v10': time_station,\n",
    "        't2': time_station,\n",
    "        'q2': time_station,\n",
    "        'hpbl': time_station,\n",
    "        'u': time_station_levmid,\n",
    "        'v': time_station_levmid,\n",
    "        't': time_station_levmid,\n",
    "        'q': time_station_levmid,\n",
    "        'p': time_station_levmid,\n",
    "        'omega': time_station_levmid,\n",
    "        'dtdt': time_station_levmid,\n",
    "        'dqdt': time_station_levmid,\n",
    "    }\n",
    "\n",
    "    # Define attributes for each variable\n",
    "    variable_attributes = {\n",
    "        'date': {'units': 'none', 'long_name': 'YYYYMMDDHH (UTC)'},\n",
    "        'latitude': {'units': 'degrees_north', 'long_name': 'latitude of station'},\n",
    "        'longitude': {'units': 'degrees_east', 'long_name': 'longitude of station'},\n",
    "        'zsfc': {'units': 'm', 'long_name': 'model surface height for the station'},\n",
    "        'psfc': {'units': 'Pa', 'long_name': 'model surface pressure for the station'},\n",
    "        'tsfc': {'units': 'K', 'long_name': 'model surface temperature'},\n",
    "        'u10': {'units': 'm/s', 'long_name': 'model derived 10-meter zonal wind'},\n",
    "        'v10': {'units': 'm/s', 'long_name': 'model derived 10-meter meridional wind'},\n",
    "        't2': {'units': 'K', 'long_name': 'model derived 2-meter temperature'},\n",
    "        'q2': {'units': 'kg/kg', 'long_name': 'model derived 2-meter specific humidity'},\n",
    "        'hpbl': {'units': 'm', 'long_name': 'model diagnosed planetary boundary layer depth'},\n",
    "        'u': {'units': 'm/s', 'long_name': 'model zonal wind velocity'},\n",
    "        'v': {'units': 'm/s', 'long_name': 'model meridional wind velocity'},\n",
    "        't': {'units': 'K', 'long_name': 'model temperature'},\n",
    "        'q': {'units': 'kg/kg', 'long_name': 'model specific humidity'},\n",
    "        'p': {'units': 'Pa', 'long_name': 'model pressure'},\n",
    "        'omega': {'units': 'Pa/s', 'long_name': 'model derived omega'},\n",
    "        'dtdt': {'units': 'K/s', 'long_name': 'model derived dtdt (advection)'},\n",
    "        'dqdt': {'units': 'kg/kg/s', 'long_name': 'model derived dqdt (advection)'},\n",
    "        'sigl': {'units': '1', 'long_name': 'model sigma level at midpoint (count upward)'},\n",
    "        'sigi': {'units': '1', 'long_name': 'model sigma level at interface (count upward)'},\n",
    "        'ak5': {'units': '1', 'long_name': 'A-coefficient'},\n",
    "        'bk5': {'units': '1', 'long_name': 'B-coefficient'},\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Add the data variables to the Dataset\n",
    "    for var_name, dims in variable_specs.items():\n",
    "        shape = tuple(ds.coords[dim].size for dim in dims)\n",
    "        ds[var_name] = xr.DataArray(np.random.rand(*shape), dims=dims)\n",
    "        ds[var_name].attrs = variable_attributes.get(var_name, {})\n",
    "\n",
    "    # Define global attributes\n",
    "    global_attributes = {\n",
    "        'title': 'NCAP GFS Single Column Model Forcing Data for VOCALS-Rex',\n",
    "        'data_source': 'NCAR/UCAR EOL - VOCALS: NCEP GFS Single Column Model Forcing Data, https://data.eol.ucar.edu/dataset/89.105',\n",
    "        'data_reference': 'https://data.eol.ucar.edu/file/download/41B38ABB023/NCEP_GFS_Single_Column_Model_Forcing_Data_for_VOCALS_Rex.pdf',\n",
    "        'history': f'Created on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n",
    "        'author': 'Yi-Hsuan Chen (yihsuanc@gate.sinica.edu.tw)'\n",
    "    }\n",
    "\n",
    "    # Assign global attributes to the Dataset\n",
    "    ds.attrs = global_attributes\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# Example usage\n",
    "ds = create_xarray_dataset()\n",
    "#ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3947421-2c7e-4f00-a6ee-8688dd1b4746",
   "metadata": {},
   "source": [
    "## process_files_in_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b25b58e5-1c62-4682-a721-9b5a434aa542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def process_files_in_directory(directory='/lfs/home/yihsuanc/data/data.TaiESM1_scm/iop/VOCALS-REx/original/'):\n",
    "\n",
    "    file_names = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('vocalsgfs'):\n",
    "            #print(filename)\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            file_names.append(file_path)\n",
    "    return sorted(file_names)\n",
    "\n",
    "#file_names = process_files_in_directory()\n",
    "#for ff in file_names:\n",
    "#    print(ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c202f-9262-4966-95dc-3a47e0ba9a24",
   "metadata": {},
   "source": [
    "# Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f66c09-8eb9-4063-b391-670913bf5b14",
   "metadata": {},
   "source": [
    "## Read a single date, all 25 stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f3abfa1a-a006-436a-839a-4021e351c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': (1,), 'station': (25,), 'latitude': (25,), 'longitude': (25,), 'sigi': (65,), 'ak5': (65,), 'bk5': (65,), 'sigl': (64,), 'zsfc': (1, 25), 'psfc': (1, 25), 'tsfc': (1, 25), 'u10': (1, 25), 'v10': (1, 25), 't2': (1, 25), 'q2': (1, 25), 'hpbl': (1, 25), 'u': (1, 25, 64), 'v': (1, 25, 64), 't': (1, 25, 64), 'q': (1, 25, 64), 'p': (1, 25, 64), 'omega': (1, 25, 64), 'dtdt': (1, 25, 64), 'dqdt': (1, 25, 64)}\n"
     ]
    }
   ],
   "source": [
    "filename = '../original/vocalsgfs.2008100100'\n",
    "data1 = read_data(filename, do_print=False)\n",
    "\n",
    "variable_dimensions = {var: arr.shape for var, arr in data1.items()}\n",
    "print(variable_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5c58c-48f7-44c0-bbf6-1c47c9609e77",
   "metadata": {},
   "source": [
    "## Read all dates, all 25 stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "8eaa044e-6eb4-4b65-ab97-8dc88e8da97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (time: 122, station: 25, lev_mid: 64, lev_int: 65)\n",
      "Coordinates:\n",
      "  * time       (time) float64 2.008e+09 2.008e+09 ... 2.008e+09 2.008e+09\n",
      "  * station    (station) int64 1 2 3 4 5 6 7 8 9 ... 17 18 19 20 21 22 23 24 25\n",
      "  * lev_mid    (lev_mid) float64 0.9973 0.9917 0.9852 ... 0.00101 0.0003212\n",
      "  * lev_int    (lev_int) float64 1.0 0.9947 0.9886 ... 0.001378 0.0006425 0.0\n",
      "Data variables: (12/23)\n",
      "    date       (time) float64 2.008e+09 2.008e+09 ... 2.008e+09 2.008e+09\n",
      "    latitude   (station) float64 -20.0 -20.0 -20.0 -20.0 ... -28.0 -30.0 -23.5\n",
      "    longitude  (station) float64 -95.0 -92.5 -90.0 -87.25 ... -85.0 -85.0 -70.0\n",
      "    sigl       (lev_mid) float64 0.9973 0.9917 0.9852 ... 0.00101 0.0003212\n",
      "    sigi       (lev_int) float64 1.0 0.9947 0.9886 ... 0.001378 0.0006425 0.0\n",
      "    ak5        (lev_int) float64 0.0 0.06425 0.1378 0.222 ... 0.000575 0.0 0.0\n",
      "    ...         ...\n",
      "    t          (time, station, lev_mid) float64 292.8 292.4 ... 264.2 236.2\n",
      "    q          (time, station, lev_mid) float64 0.01001 0.00981 ... 6.779e-07\n",
      "    p          (time, station, lev_mid) float64 1.019e+08 ... 2.666e+04\n",
      "    omega      (time, station, lev_mid) float64 7.672 6.659 ... -0.1167 0.1691\n",
      "    dtdt       (time, station, lev_mid) float64 4.564e-05 ... 0.0003126\n",
      "    dqdt       (time, station, lev_mid) float64 -1.243e-08 ... 1.487e-12\n",
      "Attributes:\n",
      "    title:           NCAP GFS Single Column Model Forcing Data for VOCALS-Rex\n",
      "    data_source:     NCAR/UCAR EOL - VOCALS: NCEP GFS Single Column Model For...\n",
      "    data_reference:  https://data.eol.ucar.edu/file/download/41B38ABB023/NCEP...\n",
      "    history:         Created on 2024-06-02 04:31:15\n",
      "    author:          Yi-Hsuan Chen (yihsuanc@gate.sinica.edu.tw)\n"
     ]
    }
   ],
   "source": [
    "#--- get all vocalsgfs file paths\n",
    "file_names = process_files_in_directory()\n",
    "\n",
    "#--- Create an xarray dataset\n",
    "ds_all = create_xarray_dataset()\n",
    "\n",
    "#--- process file_names  and then save into ds_all\n",
    "for i, ff in enumerate(file_names):\n",
    "    #print(f'Read [{i}, {ff}]')\n",
    "    data = read_data(ff)    \n",
    "\n",
    "    for var_name in data.keys():\n",
    "        if var_name in ds_all.variables:\n",
    "            ndim = data[var_name].ndim\n",
    "            #print(f\"Variable: {var_name}, ndim: {ndim}\")\n",
    "    \n",
    "            if (ndim == 1 and var_name == 'date'):\n",
    "               ds_all[var_name][i] = data[var_name][0]\n",
    "            if (ndim == 2):\n",
    "               ds_all[var_name][i,:] = data[var_name][0,:]\n",
    "            elif (ndim == 3):\n",
    "               ds_all[var_name][i,:,:] = data[var_name][0,:,:]\n",
    "\n",
    "    var_1d = ['latitude', 'longitude', 'sigl', 'sigi', 'ak5', 'bk5']\n",
    "    for var_name in var_1d:\n",
    "        ds_all[var_name][:] = data[var_name][:]\n",
    "\n",
    "#--- change coordinate\n",
    "ds_all = ds_all.assign_coords(time=ds_all['date'].values)\n",
    "ds_all.coords['time'].attrs = ds_all['date'].attrs\n",
    "\n",
    "ds_all = ds_all.assign_coords(lev_mid=ds_all['sigl'][:].values)\n",
    "ds_all.coords['lev_mid'].attrs = ds_all['sigl'].attrs\n",
    "\n",
    "ds_all = ds_all.assign_coords(lev_int=ds_all['sigi'][:].values)\n",
    "ds_all.coords['lev_int'].attrs = ds_all['sigi'].attrs\n",
    "\n",
    "print(ds_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04317108-d785-4a8a-ab05-02d725b421c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
