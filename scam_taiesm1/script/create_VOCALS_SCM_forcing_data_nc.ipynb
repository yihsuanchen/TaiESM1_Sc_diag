{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323157ca",
   "metadata": {},
   "source": [
    "# Program - Read VOCALS-REx GFS SCM forcing data (binary) and save it into a new netCDF file\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "Read VOCALS-REx GFS SCM forcing data and save it into a new netCDF file\n",
    "\n",
    "**Data**\n",
    "\n",
    "- NCAR/UCAR EOL -  VOCALS: NCEP GFS Single Column Model Forcing Data\n",
    "  - https://data.eol.ucar.edu/dataset/89.105\n",
    "- Data access\n",
    "  - ORDER data for delivery by FTP\n",
    "- Documentation\n",
    "  - https://data.eol.ucar.edu/file/download/41B38ABB023/NCEP_GFS_Single_Column_Model_Forcing_Data_for_VOCALS_Rex.pdf\n",
    "\n",
    "**Author:** Yi-Hsuan Chen (yihsuan@umich.edu)\n",
    "\n",
    "**Date:** May 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22208803-04be-4d2f-b899-447072a095af",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e61034-6e49-4ab4-b1cb-a4e61dfbdce9",
   "metadata": {},
   "source": [
    "## read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9c093ae2-d706-4d9e-9b74-a5253772e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "####################\n",
    "####################\n",
    "####################\n",
    "def create_return_arrays(num_time_steps, npoint=25, levs=64):\n",
    "\n",
    "    variable_shapes = {\n",
    "\n",
    "    #--- 1d variable, (time)\n",
    "    'date': (num_time_steps),\n",
    "\n",
    "    #--- 1d variable, (station)\n",
    "    'station': (npoint),\n",
    "    'latitude': (npoint),\n",
    "    'longitude': (npoint),\n",
    "\n",
    "    #--- 1d variable, (levs/levs+1)\n",
    "    'sigi': (levs+1),\n",
    "    'ak5': (levs+1),\n",
    "    'bk5': (levs+1),\n",
    "    'sigl': (levs),\n",
    "        \n",
    "    #--- 2d variable, (time, station)\n",
    "    'zsfc': (num_time_steps, npoint),\n",
    "    'psfc': (num_time_steps, npoint),\n",
    "    'tsfc': (num_time_steps, npoint),\n",
    "    'u10': (num_time_steps, npoint),\n",
    "    'v10': (num_time_steps, npoint),\n",
    "    't2': (num_time_steps, npoint),\n",
    "    'q2': (num_time_steps, npoint),\n",
    "    'hpbl': (num_time_steps, npoint),\n",
    "        \n",
    "    #--- 3d variable, (time, station, levels)\n",
    "    'u': (num_time_steps, npoint, levs),\n",
    "    'v': (num_time_steps, npoint, levs),\n",
    "    't': (num_time_steps, npoint, levs),\n",
    "    'q': (num_time_steps, npoint, levs),\n",
    "    'p': (num_time_steps, npoint, levs),\n",
    "    'omega': (num_time_steps, npoint, levs),\n",
    "    'dtdt': (num_time_steps, npoint, levs),\n",
    "    'dqdt': (num_time_steps, npoint, levs),\n",
    "}\n",
    "\n",
    "    data_arrays = {var: np.zeros(shape) for var, shape in variable_shapes.items()}\n",
    "\n",
    "    #--- set a new string variable\n",
    "    #new_string_var = [['' for _ in range(npoint)] for _ in range(num_time_steps)]\n",
    "    #data_arrays['date_string'] = new_string_var\n",
    "\n",
    "    # Initialize the 'station' variable as a list of lists (strings)\n",
    "    #data_arrays['station_string'] = [['' for _ in range(1)] for _ in range(npoint)]\n",
    "    \n",
    "    return data_arrays\n",
    "\n",
    "####################\n",
    "####################\n",
    "####################\n",
    "def read_data(filename, \n",
    "              do_print=False):\n",
    "    with open(filename, 'rb') as file:\n",
    "        # Read the header\n",
    "        header_format = '>13i'\n",
    "        header_size = struct.calcsize(header_format)\n",
    "        header_data = struct.unpack(header_format, file.read(header_size))\n",
    "        \n",
    "        # Unpack the header data\n",
    "        uu, hour, month, day, year, nsfc, nflx, nvar, levs, npoint, start_hour, end_hour, step_hour = header_data\n",
    "\n",
    "        if (do_print):\n",
    "            print(f'Hour: {hour}, Month: {month}, Day: {day}, Year: {year}')\n",
    "            print(f'Number of surface variables: {nsfc}')\n",
    "            print(f'Number of flux variables: {nflx}')\n",
    "            print(f'Number of variables for each sounding: {nvar}')\n",
    "            print(f'Number of vertical levels: {levs}')\n",
    "            print(f'Number of station points: {npoint}')\n",
    "            print(f'Starting forecast hour: {start_hour}')\n",
    "            print(f'Ending forecast hour: {end_hour}')\n",
    "            print(f'Forecast output step: {step_hour}')\n",
    "\n",
    "        #--- create return data array\n",
    "        data = create_return_arrays(num_time_steps=1, npoint=npoint, levs=levs)\n",
    "        time_step = 0\n",
    "        \n",
    "        # Skip the first two values before reading sigi\n",
    "        file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "        \n",
    "        # Read the second record of vertical sounding levels\n",
    "        sigi = np.fromfile(file, dtype='>f4', count=levs + 1)\n",
    "        sigl = np.fromfile(file, dtype='>f4', count=levs)\n",
    "        ak5 = np.fromfile(file, dtype='>f4', count=levs + 1)\n",
    "        bk5 = np.fromfile(file, dtype='>f4', count=levs + 1)\n",
    "            \n",
    "        # Calculate the number of time steps\n",
    "        num_time_steps = (end_hour - start_hour) // step_hour + 1\n",
    "\n",
    "        # Loop over station points\n",
    "        for i in range(npoint):\n",
    "\n",
    "            # Skip the first two values before reading surface_data\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "\n",
    "            # Read surface variables\n",
    "            surface_vars_format = f'>{nsfc}f'\n",
    "            #surface_vars_format = f'>50f'\n",
    "            surface_vars_size = struct.calcsize(surface_vars_format)\n",
    "            surface_data = struct.unpack(surface_vars_format, file.read(surface_vars_size))\n",
    "\n",
    "            # Print each element in surface_data\n",
    "            #print(f'Surface Data for Station {i+1}:')                 \n",
    "            if (do_print):\n",
    "                for j, value in enumerate(surface_data):\n",
    "                    print(f'surface_data {j}: {value}')\n",
    "\n",
    "            # Skip the first two values before reading flux_data\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "            \n",
    "            # Read flux type variables if nflx > 0\n",
    "            if nflx > 0:\n",
    "                flux_vars_format = f'>{nflx}f'\n",
    "                flux_vars_size = struct.calcsize(flux_vars_format)\n",
    "                flux_data = struct.unpack(flux_vars_format, file.read(flux_vars_size))\n",
    "\n",
    "                # Print flux data for debugging purposes\n",
    "                if (do_print):\n",
    "                    for j, value in enumerate(flux_data):\n",
    "                        print(f'flux_data {j}: {value}')\n",
    "                        \n",
    "            # Read vertical levels data\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            u = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            v = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            t = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            q = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            file.seek(8, 1)  # Skip 8 bytes (2 values)  \n",
    "            p = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            if nvar > 5:\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                omega = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                dtdt = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                dqdt = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            if nvar > 8:\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                cloud_water = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                cloud_water_tend = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "                file.seek(8, 1)  # Skip 8 bytes (2 values)\n",
    "                cloud_fraction = np.fromfile(file, dtype='>f4', count=levs)\n",
    "\n",
    "            #--- save variables into data array\n",
    "\n",
    "            str1 = f\"{year}{month}{day:02}{hour:02}\"\n",
    "            data['date'][time_step] = str1\n",
    "            data['station'][i] = f\"{i+1:02}\"\n",
    "\n",
    "            #--- 1d variable \n",
    "            data['latitude'][i] = surface_data[0]\n",
    "            data['longitude'][i] = surface_data[1]\n",
    "            data['sigi'][:] = sigi\n",
    "            data['sigl'][:] = sigl\n",
    "            data['ak5'][:] = ak5\n",
    "            data['bk5'][:] = bk5\n",
    "            \n",
    "            #--- 2d variable, (time, station)\n",
    "            data['zsfc'][time_step, i] = surface_data[2]\n",
    "            data['psfc'][time_step, i] = surface_data[3]\n",
    "            data['tsfc'][time_step, i] = surface_data[4]\n",
    "            \n",
    "            data['u10'][time_step, i] = flux_data[17]\n",
    "            data['v10'][time_step, i] = flux_data[18]\n",
    "            data['t2'][time_step, i] = flux_data[19]\n",
    "            data['q2'][time_step, i] = flux_data[20]\n",
    "            data['hpbl'][time_step, i] = flux_data[26]\n",
    "\n",
    "            #--- 3d variable, (time, station, levs/levs+1)\n",
    "            data['u'][time_step, i, :] = u\n",
    "            data['v'][time_step, i, :] = v\n",
    "            data['t'][time_step, i, :] = t\n",
    "            data['q'][time_step, i, :] = q\n",
    "            data['p'][time_step, i, :] = p\n",
    "            data['omega'][time_step, i, :] = omega\n",
    "            data['dtdt'][time_step, i, :] = dtdt\n",
    "            data['dqdt'][time_step, i, :] = dqdt\n",
    "            #data[''][time_step, i, :] = \n",
    "\n",
    "        if (do_print):\n",
    "            print('Sigi:', sigi)\n",
    "            print('Sigl:', sigl)\n",
    "            print('Ak5:', ak5)\n",
    "            print('Bk5:', bk5)\n",
    "            print('u:', u)\n",
    "            print('v:', v)\n",
    "            print('t:', t)\n",
    "            print('q:', q)\n",
    "            print('omega:', omega*1e+5/86400)\n",
    "            print('dtdt', dtdt)\n",
    "            print('dqdt', dtdt)\n",
    "            print('cloud_water', cloud_water)\n",
    "            print('cloud_water_tend', cloud_water_tend)\n",
    "            print('cloud_fraction', cloud_fraction)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "filename = '../original/vocalsgfs.2008101100'\n",
    "\n",
    "#data = read_data(filename, do_print=True)\n",
    "data = read_data(filename, do_print=False)\n",
    "\n",
    "#data['q2']\n",
    "#data['p'][0,11,:]\n",
    "#data['latitude']\n",
    "#print(data['date'].astype(int))\n",
    "#print(data['station'])\n",
    "\n",
    "#data\n",
    "\n",
    "#data_all = create_return_arrays(num_time_steps=2, npoint=25, levs=64)\n",
    "\n",
    "#filename = '../original/vocalsgfs.2008101100'\n",
    "#read_data(filename, data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496fbf26-f4e6-4bf9-972c-736a1f6f5439",
   "metadata": {},
   "source": [
    "## create_xarray_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fa8d80b0-335a-44ec-9b2c-cec733e11325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def create_xarray_dataset(num_time_steps=122, num_station=25, num_levels=64):\n",
    "    # Define the coordinate arrays\n",
    "    times = np.arange(num_time_steps)\n",
    "    stations = np.arange(1, num_station+1)\n",
    "    levels_mid = np.arange(num_levels)\n",
    "    levels_int = np.arange(num_levels + 1)\n",
    "\n",
    "    # Create the xarray Dataset with coordinates\n",
    "    ds = xr.Dataset(\n",
    "        coords={\n",
    "            'time': ('time', times),\n",
    "            'station': ('station', stations),\n",
    "            'lev_mid': ('lev_mid', levels_mid),\n",
    "            'lev_int': ('lev_int', levels_int),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Define attributes for coordinates\n",
    "    coordinate_attributes = {\n",
    "        'time': {'units': 'hours since 2024-01-01 00:00:00', 'long_name': 'time'},\n",
    "        'station': {'units': '1', 'long_name': 'station index, VOCALS[##]'},\n",
    "        'lev_mid': {'units': '1', 'long_name': 'sigma level at midpoint (start from the near surface level)'},\n",
    "        'lev_int': {'units': '1', 'long_name': 'sigma level at interface (start from the near surface level)'},\n",
    "    }\n",
    "\n",
    "    # Assign attributes to coordinates\n",
    "    for coord_name, attrs in coordinate_attributes.items():\n",
    "        ds.coords[coord_name].attrs = attrs\n",
    "\n",
    "    # Define the dimension tuples\n",
    "    time_only = ('time',)\n",
    "    station_only = ('station',)\n",
    "    lev_mid_only = ('lev_mid',)\n",
    "    lev_int_only = ('lev_int',)\n",
    "    time_station = ('time', 'station')\n",
    "    time_station_levmid = ('time', 'station', 'lev_mid')\n",
    "    time_station_levint = ('time', 'station', 'lev_int')\n",
    "\n",
    "    # Define the data variables and their dimensions\n",
    "    variable_specs = {\n",
    "        'date': time_only,\n",
    "        'latitude': station_only,\n",
    "        'longitude': station_only,\n",
    "        'sigl': lev_mid_only,\n",
    "        'sigi': lev_int_only,\n",
    "        'ak5': lev_int_only,\n",
    "        'bk5': lev_int_only,\n",
    "        'zsfc': time_station,\n",
    "        'psfc': time_station,\n",
    "        'tsfc': time_station,\n",
    "        'u10': time_station,\n",
    "        'v10': time_station,\n",
    "        't2': time_station,\n",
    "        'q2': time_station,\n",
    "        'hpbl': time_station,\n",
    "        'u': time_station_levmid,\n",
    "        'v': time_station_levmid,\n",
    "        't': time_station_levmid,\n",
    "        'q': time_station_levmid,\n",
    "        'p': time_station_levmid,\n",
    "        'omega': time_station_levmid,\n",
    "        'dtdt': time_station_levmid,\n",
    "        'dqdt': time_station_levmid,\n",
    "    }\n",
    "\n",
    "    # Define attributes for each variable\n",
    "    variable_attributes = {\n",
    "        'date': {'units': 'none', 'long_name': 'YYYYMMDDHH (UTC)'},\n",
    "        'latitude': {'units': 'degrees_north', 'long_name': 'latitude of station'},\n",
    "        'longitude': {'units': 'degrees_east', 'long_name': 'longitude of station'},\n",
    "        'zsfc': {'units': 'm', 'long_name': 'model surface height for the station'},\n",
    "        'psfc': {'units': 'Pa', 'long_name': 'model surface pressure for the station'},\n",
    "        'tsfc': {'units': 'K', 'long_name': 'model surface temperature'},\n",
    "        'u10': {'units': 'm/s', 'long_name': 'model derived 10-meter zonal wind'},\n",
    "        'v10': {'units': 'm/s', 'long_name': 'model derived 10-meter meridional wind'},\n",
    "        't2': {'units': 'K', 'long_name': 'model derived 2-meter temperature'},\n",
    "        'q2': {'units': 'kg/kg', 'long_name': 'model derived 2-meter specific humidity'},\n",
    "        'hpbl': {'units': 'm', 'long_name': 'model diagnosed planetary boundary layer depth'},\n",
    "        'u': {'units': 'm/s', 'long_name': 'model zonal wind velocity'},\n",
    "        'v': {'units': 'm/s', 'long_name': 'model meridional wind velocity'},\n",
    "        't': {'units': 'K', 'long_name': 'model temperature'},\n",
    "        'q': {'units': 'kg/kg', 'long_name': 'model specific humidity'},\n",
    "        'p': {'units': 'Pa', 'long_name': 'model pressure'},\n",
    "        'omega': {'units': 'Pa/s', 'long_name': 'model derived omega'},\n",
    "        'dtdt': {'units': 'K/s', 'long_name': 'model derived dtdt (advection)'},\n",
    "        'dqdt': {'units': 'kg/kg/s', 'long_name': 'model derived dqdt (advection)'},\n",
    "        'sigl': {'units': '1', 'long_name': 'model sigma level at midpoint (count upward)'},\n",
    "        'sigi': {'units': '1', 'long_name': 'model sigma level at interface (count upward)'},\n",
    "        'ak5': {'units': '1', 'long_name': 'A-coefficient'},\n",
    "        'bk5': {'units': '1', 'long_name': 'B-coefficient'},\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Add the data variables to the Dataset\n",
    "    for var_name, dims in variable_specs.items():\n",
    "        shape = tuple(ds.coords[dim].size for dim in dims)\n",
    "        ds[var_name] = xr.DataArray(np.random.rand(*shape), dims=dims)\n",
    "        ds[var_name].attrs = variable_attributes.get(var_name, {})\n",
    "\n",
    "    # Define global attributes\n",
    "    global_attributes = {\n",
    "        'title': 'NCAP GFS Single Column Model Forcing Data for VOCALS-Rex',\n",
    "        'data_source': 'NCAR/UCAR EOL - VOCALS: NCEP GFS Single Column Model Forcing Data, https://data.eol.ucar.edu/dataset/89.105',\n",
    "        'data_reference': 'https://data.eol.ucar.edu/file/download/41B38ABB023/NCEP_GFS_Single_Column_Model_Forcing_Data_for_VOCALS_Rex.pdf',\n",
    "        'history': f'Created on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n",
    "        'author': 'Yi-Hsuan Chen (yihsuanc@gate.sinica.edu.tw)'\n",
    "    }\n",
    "\n",
    "    # Assign global attributes to the Dataset\n",
    "    ds.attrs = global_attributes\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# Example usage\n",
    "ds = create_xarray_dataset()\n",
    "#ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3947421-2c7e-4f00-a6ee-8688dd1b4746",
   "metadata": {},
   "source": [
    "## process_files_in_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b25b58e5-1c62-4682-a721-9b5a434aa542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def process_files_in_directory(directory='/lfs/home/yihsuanc/data/data.TaiESM1_scm/iop/VOCALS-REx/original/'):\n",
    "\n",
    "    file_names = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('vocalsgfs'):\n",
    "            #print(filename)\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            file_names.append(file_path)\n",
    "    return sorted(file_names)\n",
    "\n",
    "#file_names = process_files_in_directory()\n",
    "#for ff in file_names:\n",
    "#    print(ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff51050-e94a-4ab4-934c-fe79fb30f675",
   "metadata": {},
   "source": [
    "## read_data_all_to_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "68223b30-fb92-4329-aca0-cbd0859b11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_all_to_ds():\n",
    "    #--- get all vocalsgfs file paths\n",
    "    file_names = process_files_in_directory()\n",
    "    \n",
    "    #--- Create an xarray dataset\n",
    "    ds_all = create_xarray_dataset()\n",
    "    \n",
    "    #--- process file_names  and then save into ds_all\n",
    "    for i, ff in enumerate(file_names):\n",
    "        #print(f'Read [{i}, {ff}]')\n",
    "        data = read_data(ff)    \n",
    "    \n",
    "        for var_name in data.keys():\n",
    "            if var_name in ds_all.variables:\n",
    "                ndim = data[var_name].ndim\n",
    "                #print(f\"Variable: {var_name}, ndim: {ndim}\")\n",
    "        \n",
    "                if (ndim == 1 and var_name == 'date'):\n",
    "                   ds_all[var_name][i] = data[var_name][0]\n",
    "                if (ndim == 2):\n",
    "                   ds_all[var_name][i,:] = data[var_name][0,:]\n",
    "                elif (ndim == 3):\n",
    "                   ds_all[var_name][i,:,:] = data[var_name][0,:,:]\n",
    "    \n",
    "        var_1d = ['latitude', 'longitude', 'sigl', 'sigi', 'ak5', 'bk5']\n",
    "        for var_name in var_1d:\n",
    "            ds_all[var_name][:] = data[var_name][:]\n",
    "    \n",
    "    #--- change coordinate\n",
    "    ds_all = ds_all.assign_coords(time=ds_all['date'].values)\n",
    "    ds_all.coords['time'].attrs = ds_all['date'].attrs\n",
    "    \n",
    "    ds_all = ds_all.assign_coords(lev_mid=ds_all['sigl'][:].values)\n",
    "    ds_all.coords['lev_mid'].attrs = ds_all['sigl'].attrs\n",
    "    \n",
    "    ds_all = ds_all.assign_coords(lev_int=ds_all['sigi'][:].values)\n",
    "    ds_all.coords['lev_int'].attrs = ds_all['sigi'].attrs\n",
    "\n",
    "    return ds_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c202f-9262-4966-95dc-3a47e0ba9a24",
   "metadata": {},
   "source": [
    "# Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f66c09-8eb9-4063-b391-670913bf5b14",
   "metadata": {},
   "source": [
    "## Read a single date, all 25 stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f3abfa1a-a006-436a-839a-4021e351c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': (1,), 'station': (25,), 'latitude': (25,), 'longitude': (25,), 'sigi': (65,), 'ak5': (65,), 'bk5': (65,), 'sigl': (64,), 'zsfc': (1, 25), 'psfc': (1, 25), 'tsfc': (1, 25), 'u10': (1, 25), 'v10': (1, 25), 't2': (1, 25), 'q2': (1, 25), 'hpbl': (1, 25), 'u': (1, 25, 64), 'v': (1, 25, 64), 't': (1, 25, 64), 'q': (1, 25, 64), 'p': (1, 25, 64), 'omega': (1, 25, 64), 'dtdt': (1, 25, 64), 'dqdt': (1, 25, 64)}\n"
     ]
    }
   ],
   "source": [
    "filename = '../original/vocalsgfs.2008100100'\n",
    "data1 = read_data(filename, do_print=False)\n",
    "\n",
    "variable_dimensions = {var: arr.shape for var, arr in data1.items()}\n",
    "print(variable_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5c58c-48f7-44c0-bbf6-1c47c9609e77",
   "metadata": {},
   "source": [
    "## Read all dates, all 25 stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c97a6a4d-1f08-4a00-a703-35667e08be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (time: 122, station: 25, lev_mid: 64, lev_int: 65)\n",
      "Coordinates:\n",
      "  * time       (time) float64 2.008e+09 2.008e+09 ... 2.008e+09 2.008e+09\n",
      "  * station    (station) int64 1 2 3 4 5 6 7 8 9 ... 17 18 19 20 21 22 23 24 25\n",
      "  * lev_mid    (lev_mid) float64 0.9973 0.9917 0.9852 ... 0.00101 0.0003212\n",
      "  * lev_int    (lev_int) float64 1.0 0.9947 0.9886 ... 0.001378 0.0006425 0.0\n",
      "Data variables: (12/23)\n",
      "    date       (time) float64 2.008e+09 2.008e+09 ... 2.008e+09 2.008e+09\n",
      "    latitude   (station) float64 -20.0 -20.0 -20.0 -20.0 ... -28.0 -30.0 -23.5\n",
      "    longitude  (station) float64 -95.0 -92.5 -90.0 -87.25 ... -85.0 -85.0 -70.0\n",
      "    sigl       (lev_mid) float64 0.9973 0.9917 0.9852 ... 0.00101 0.0003212\n",
      "    sigi       (lev_int) float64 1.0 0.9947 0.9886 ... 0.001378 0.0006425 0.0\n",
      "    ak5        (lev_int) float64 0.0 0.06425 0.1378 0.222 ... 0.000575 0.0 0.0\n",
      "    ...         ...\n",
      "    t          (time, station, lev_mid) float64 292.8 292.4 ... 264.2 236.2\n",
      "    q          (time, station, lev_mid) float64 0.01001 0.00981 ... 6.779e-07\n",
      "    p          (time, station, lev_mid) float64 1.019e+08 ... 2.666e+04\n",
      "    omega      (time, station, lev_mid) float64 7.672 6.659 ... -0.1167 0.1691\n",
      "    dtdt       (time, station, lev_mid) float64 4.564e-05 ... 0.0003126\n",
      "    dqdt       (time, station, lev_mid) float64 -1.243e-08 ... 1.487e-12\n",
      "Attributes:\n",
      "    title:           NCAP GFS Single Column Model Forcing Data for VOCALS-Rex\n",
      "    data_source:     NCAR/UCAR EOL - VOCALS: NCEP GFS Single Column Model For...\n",
      "    data_reference:  https://data.eol.ucar.edu/file/download/41B38ABB023/NCEP...\n",
      "    history:         Created on 2024-06-02 09:12:19\n",
      "    author:          Yi-Hsuan Chen (yihsuanc@gate.sinica.edu.tw)\n"
     ]
    }
   ],
   "source": [
    "ds_all = read_data_all_to_ds()\n",
    "print(ds_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c7c9f-7abb-46dc-a338-6f1ca38c4149",
   "metadata": {},
   "source": [
    "## Check combined dataset with inividual one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "9bdb8aee-2d44-4c12-839b-4d874f76f164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_all:  -20.0\n",
      "single:  -20.0\n",
      "ds_all:  -70.0\n",
      "single:  -70.0\n",
      "psfc\n",
      "  ds_all:  92821584.0\n",
      "  ds_all - single:  0.0\n",
      "tsfc\n",
      "  ds_all:  0.0005159290740266442\n",
      "  ds_all - single:  0.0\n",
      "t2\n",
      "  ds_all:  291.2839050292969\n",
      "  ds_all - single:  0.0\n",
      "q2\n",
      "  ds_all:  0.007325334474444389\n",
      "  ds_all - single:  0.0\n",
      "t\n",
      "  ds_all:  [290.35858154 289.87283325 289.51748657 289.22332764 289.07583618\n",
      " 289.16253662 289.28857422 289.33886719 289.365448   289.30908203\n",
      " 289.15063477 288.85018921 288.3890686  287.75067139 286.92098999\n",
      " 285.86489868 284.73895264 283.41021729 281.84939575 279.86102295\n",
      " 277.28851318 273.97070312 270.33227539 266.82763672 263.58724976\n",
      " 260.27078247 257.50271606 254.74697876 250.80262756 244.98513794\n",
      " 239.7756958  233.56271362 228.3825531  222.5932312  216.68540955\n",
      " 211.48352051 205.36715698 201.81356812 196.72789001 195.54672241\n",
      " 194.63632202 197.384552   201.27037048 202.88566589 205.52885437\n",
      " 209.38372803 211.01477051 211.53338623 211.49095154 213.50582886\n",
      " 216.48968506 218.15408325 221.49659729 226.94664001 230.98352051\n",
      " 229.39508057 230.15228271 240.46960449 236.83752441 244.97840881\n",
      " 244.995224   252.81561279 264.01593018 243.90168762]\n",
      "  ds_all - single:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "u\n",
      "  ds_all:  [-2.79149741e-01 -5.66423476e-01 -8.50990236e-01 -1.13511693e+00\n",
      " -1.33991492e+00 -1.31441653e+00 -1.12570739e+00 -9.23346221e-01\n",
      " -8.08231235e-01 -7.55707204e-01 -8.40548098e-01 -1.01399481e+00\n",
      " -1.22203970e+00 -1.32987547e+00 -1.32679331e+00 -1.10116768e+00\n",
      " -6.74855173e-01 -2.33599003e-02  7.58987606e-01  1.18355525e+00\n",
      "  1.31859970e+00  2.29784584e+00  3.87338972e+00  3.77679253e+00\n",
      "  4.10694695e+00  3.65962219e+00  4.11189127e+00  5.89063263e+00\n",
      "  8.36052513e+00  1.12603111e+01  1.57127781e+01  1.54416590e+01\n",
      "  1.38510523e+01  1.11937056e+01  1.37709074e+01  1.84754467e+01\n",
      "  2.03414078e+01  2.07139454e+01  2.09777203e+01  1.07401876e+01\n",
      "  8.78631020e+00  1.15664997e+01  4.01408863e+00 -2.08015037e+00\n",
      " -2.70791078e+00 -3.87027645e+00 -6.60884714e+00 -3.29352069e+00\n",
      " -2.40100288e+00 -4.23200321e+00 -4.91603422e+00 -6.75561666e+00\n",
      " -3.59588170e+00  3.05022478e+00  7.28972864e+00  1.02867661e+01\n",
      "  1.05033894e+01  1.57336724e+00 -2.54800057e+00  1.40549648e+00\n",
      "  1.41022558e+01  2.98217564e+01  3.98241653e+01  1.43143902e+01]\n",
      "  ds_all - single:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "v\n",
      "  ds_all:  [  2.50385237   2.62747097   2.65838957   2.65390468   2.58635259\n",
      "   2.29953456   1.83139539   1.32245469   0.97785813   0.53387481\n",
      "   0.07629852  -0.34437406  -0.75045919  -1.20426571  -1.79044354\n",
      "  -2.35373354  -2.66249537  -2.73308849  -2.25358009  -1.34684944\n",
      "  -0.5066312   -2.06376004  -1.15978646  -3.1807704   -5.59032726\n",
      "  -8.50635433 -10.34548283 -10.00662136  -8.79367161  -7.65795898\n",
      "  -4.64272881  -3.76116252  -6.09647083  -8.68668556  -9.79698849\n",
      "  -6.14617109  -1.11418736  -3.12355852  -1.48586667   0.09863834\n",
      "  -3.49528909   5.13170767  10.41825485   4.57424974   5.06908464\n",
      "   0.64656901   0.96272343  -1.33101273  -1.08859468  -0.90594614\n",
      "   0.55918908   0.92971873   2.00228906   1.96840978  -3.39236355\n",
      "  -5.87787104   2.79485488   0.6278863   -3.30458522  -3.5427618\n",
      "  -1.44685388  -4.67542696  -1.87877083   0.67754668]\n",
      "  ds_all - single:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "q\n",
      "  ds_all:  [ 7.20954500e-03  7.15862634e-03  7.08922138e-03  6.97068730e-03\n",
      "  6.71105692e-03  6.22153468e-03  5.73199149e-03  5.29218605e-03\n",
      "  4.78235679e-03  4.24496550e-03  3.71577917e-03  3.22204782e-03\n",
      "  2.80503812e-03  2.51056906e-03  2.38415645e-03  2.36401148e-03\n",
      "  2.31035030e-03  2.20825267e-03  2.11073202e-03  2.47421488e-03\n",
      "  3.49365431e-03  4.57808562e-03  4.75176889e-03  4.09292057e-03\n",
      "  3.01099010e-03  1.97482761e-03  1.22026505e-03  3.58055928e-04\n",
      " -3.47611349e-05  1.91971660e-04  2.35978674e-04  2.08030688e-04\n",
      "  1.07123022e-04  9.51252587e-05  3.71681745e-05  2.23313618e-05\n",
      "  1.36392409e-05  5.55007136e-06  2.38644657e-06  8.65269158e-07\n",
      " -2.24239875e-06 -4.32195930e-06 -4.35452966e-06 -5.24497955e-06\n",
      " -8.54674909e-06 -7.86272994e-06 -9.11887219e-06 -8.27554959e-06\n",
      " -7.42527254e-06 -7.74758155e-06 -5.63268441e-06 -3.47806144e-06\n",
      " -1.70606893e-06 -4.36233108e-07 -1.45917738e-06 -1.03002469e-06\n",
      "  2.88721282e-07 -1.22508624e-07 -6.70221539e-07 -1.45285853e-07\n",
      "  1.20880216e-07  4.91278286e-07  5.29214390e-07  6.45061448e-07]\n",
      "  ds_all - single:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "p\n",
      "  ds_all:  [9.25741840e+07 9.20466000e+07 9.14494080e+07 9.07748640e+07\n",
      " 9.00147840e+07 8.91604960e+07 8.82029120e+07 8.71326640e+07\n",
      " 8.59402080e+07 8.46161200e+07 8.31513440e+07 8.15375760e+07\n",
      " 7.97676240e+07 7.78359200e+07 7.57389920e+07 7.34759520e+07\n",
      " 7.10490080e+07 6.84638400e+07 6.57299600e+07 6.28607800e+07\n",
      " 5.98736280e+07 5.67894560e+07 5.36323240e+07 5.04287120e+07\n",
      " 4.72066000e+07 4.39945000e+07 4.08203680e+07 3.77106280e+07\n",
      " 3.46892880e+07 3.17772140e+07 2.89916360e+07 2.63458960e+07\n",
      " 2.38494520e+07 2.15085760e+07 1.93269120e+07 1.73053280e+07\n",
      " 1.54421150e+07 1.37334740e+07 1.21740180e+07 1.07572280e+07\n",
      " 9.47586000e+06 8.32230000e+06 7.28883950e+06 6.36744100e+06\n",
      " 5.54891750e+06 4.82345150e+06 4.18168850e+06 3.61492850e+06\n",
      " 3.11515175e+06 2.67502150e+06 2.28786775e+06 1.94765938e+06\n",
      " 1.64897200e+06 1.38694325e+06 1.15723012e+06 9.55967625e+05\n",
      " 7.79722562e+05 6.25452375e+05 4.90464688e+05 3.72378469e+05\n",
      " 2.69083812e+05 1.78689391e+05 9.93825938e+04 2.66594082e+04]\n",
      "  ds_all - single:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "date = 2008101612\n",
    "station = 11\n",
    "\n",
    "#--- combined dataset\n",
    "ds_all.sel(time=date, station=station)\n",
    "\n",
    "#--- individual data array\n",
    "filename = f'../original/vocalsgfs.{date}'\n",
    "data2 = read_data(filename, do_print=False)\n",
    "\n",
    "#--- check lat & lon\n",
    "var_1d = ['latitude', 'longitude']\n",
    "for var_name in var_1d:\n",
    "    print('ds_all: ', ds_all[var_name].sel(station=station).values)\n",
    "    print('single: ', data2[var_name][station-1])\n",
    "\n",
    "#--- check 2d variable (time, station)\n",
    "var_2d = ['psfc','tsfc','t2','q2']\n",
    "for var_name in var_2d:\n",
    "    print(var_name)\n",
    "\n",
    "    vv1 = ds_all[var_name].sel(time=date, station=station)\n",
    "    vv2 = data2[var_name][0, station-1]\n",
    "    vv1m2 = vv1 - vv2\n",
    "    print('  ds_all: ', vv1.values)\n",
    "    print('  ds_all - single: ', vv1m2.values)\n",
    "    #print('ds_all: ', ds_all[var_name].sel(time=date, station=16).values)\n",
    "    #print('single: ', data2[var_name][0, station-1, :])\n",
    "\n",
    "#--- check 3d variable (time, station, levs)\n",
    "var_nd = ['t','u','v','q','p']\n",
    "for var_name in var_nd:\n",
    "    print(var_name)\n",
    "\n",
    "    vv1 = ds_all[var_name].sel(time=date, station=station)\n",
    "    vv2 = data2[var_name][0, station-1, :]\n",
    "    vv1m2 = vv1 - vv2\n",
    "    print('  ds_all: ', vv1.values)\n",
    "    print('  ds_all - single: ', vv1m2.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae83827-87c9-4d50-a0ee-fa85df991ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
